{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import *\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version ( Thanks Jim!)\n",
    "\n",
    "It is based on binary search as used in bisect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyhton version\n",
    "def vectorized_search(offsets, content):\n",
    "    print(\"on CUDA, this would be %d threads for as many particles\" % len(content))\n",
    "    print(\"expected convergence in %g steps\" % numpy.log2(len(offsets) - 1))\n",
    "\n",
    "    index = numpy.arange(len(content), dtype=int)                     # threadIdx.x on CUDA\n",
    "    below = numpy.zeros(len(content), dtype=int)                      # just below = 0 on CUDA\n",
    "    above = numpy.ones(len(content), dtype=int) * (len(offsets) - 1)  # same for above\n",
    "\n",
    "    step = 0   # only used for print-outs\n",
    "    while True:\n",
    "        middle = (below + above) // 2\n",
    "\n",
    "        step += 1\n",
    "        print(\"step %d: try parents = %s\" % (step, str(middle)))\n",
    "\n",
    "        change_below = offsets[middle + 1] <= index                   # which \"belows\" must we change?\n",
    "        change_above = offsets[middle] > index                        # which \"aboves\"?\n",
    "\n",
    "        if not numpy.bitwise_or(change_below, change_above).any():    # neither? great! we're done!\n",
    "            break\n",
    "        else:\n",
    "            below = numpy.where(change_below, middle + 1, below)      # vectorized \"if\" statement\n",
    "            above = numpy.where(change_above, middle - 1, above)      # this is the only branch\n",
    "\n",
    "    print(\"done!\")\n",
    "    return middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = numpy.array([0, 3, 3, 5, 9])\n",
    "content = numpy.array([1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on CUDA, this would be 9 threads for as many particles\n",
      "expected convergence in 2 steps\n",
      "step 1: try parents = [2 2 2 2 2 2 2 2 2]\n",
      "step 2: try parents = [0 0 0 2 2 3 3 3 3]\n",
      "done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 2, 3, 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_search(offsets,content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) CUDA Loop version\n",
    "\n",
    "There is an assumption that `index` is within `len(content)`, but it's easy to evade that, as done many times earlier. \n",
    "\n",
    "A similar thing is done in block and grid size determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule('''\n",
    "__global__ void vectorized_search(int* offsets, int* middle,int* len_content,int* below,int* above)\n",
    "{\n",
    "    int index = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    \n",
    "    while (1)\n",
    "    {\n",
    "        middle[index] = int((below[index] + above[index])/2);\n",
    "        if (offsets[middle[index]+1]<=index || offsets[middle[index]]>index)\n",
    "        {\n",
    "            below[index] = (offsets[middle[index]+1]<=index)? middle[index]+1 :below[index];\n",
    "            above[index] = (offsets[middle[index]]>index) ? middle[index]-1: above[index];\n",
    "        }\n",
    "        else\n",
    "            break;\n",
    "    }\n",
    "}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"vectorized_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_offsets = gpuarray.to_gpu(offsets)\n",
    "g_len_content = gpuarray.to_gpu(numpy.array(len(content), dtype=numpy.int))\n",
    "below = gpuarray.zeros(len(content), dtype=numpy.int)\n",
    "above = gpuarray.zeros(len(content), dtype=numpy.int) + (len(offsets)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle = gpuarray.empty(len(content), dtype=numpy.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(g_offsets,middle, g_len_content, below, above, block=(len(content),1,1), grid=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the result\n",
    "middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Python loop with gpuarrays\n",
    "\n",
    "The loop here is in python. However, the arrays are gpuarray instances.\n",
    "This is not very useful, as pycuda doesn't have support for array indexing, and implementing it will result in the same source module as above. \n",
    "Currently doing with a mix of elementwise and reduction kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def pycuda_where(mask, arr1, arr2):\n",
    "    # Simulate numpy.where in PyCUDA gpuarrays\n",
    "    mod = SourceModule('''\n",
    "    __global__ void where(bool* mask,int* arr1,int* arr2,int* out)\n",
    "    {\n",
    "        int index = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "        out[index] = mask[index]?arr1[index]:arr2[index];\n",
    "    }\n",
    "    ''')\n",
    "    py_where = mod.get_function(\"where\")\n",
    "    \n",
    "    if len(arr1) != len(arr2):\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        data_len = len(arr1)\n",
    "        out = gpuarray.empty_like(arr1)\n",
    "        if (data_len < 512):\n",
    "            thread_size = data_len-1\n",
    "            block_size = 1\n",
    "        else:\n",
    "            thread_size = 512-1\n",
    "            block_size = data_len//512\n",
    "        py_where(mask, arr1, arr2, out, block=(thread_size,1,1), grid=(block_size,1,1))\n",
    "        return out\n",
    "\n",
    "def gpuarray_search(offsets, content):\n",
    "    index = gpuarray.arange(len(content), dtype=numpy.int)\n",
    "    below = gpuarray.zeros(len(content), dtype=numpy.int)\n",
    "    above = gpuarray.zeros_like(below) + len(offsets)-1\n",
    "    # Additional gpuarrays needed\n",
    "    middle = gpuarray.empty_like(below)\n",
    "    g_offsets = gpuarray.to_gpu(offsets)\n",
    "    change_below = gpuarray.empty(len(content), dtype=numpy.bool)\n",
    "    change_above = gpuarray.empty(len(content), dtype=numpy.bool)\n",
    "    # Additional kernels needed for integer division\n",
    "    div_kern = pycuda.elementwise.ElementwiseKernel(\"int* a,int* b, int* out\",\n",
    "                                                   \"out[i] = int((a[i]+b[i])/2)\",\n",
    "                                                   \"mod_kern\")\n",
    "    comp_kern1 = pycuda.elementwise.ElementwiseKernel(\"bool* out,int* offsets,int* middle\",\n",
    "                                                    \"out[i] = offsets[middle[i]+1]<=i\",\n",
    "                                                    \"comp_kern1\")\n",
    "    comp_kern2 = pycuda.elementwise.ElementwiseKernel(\"bool* out,int* offsets,int* middle\",\n",
    "                                                    \"out[i] = offsets[middle[i]]>i\",\n",
    "                                                    \"comp_kern2\")\n",
    "    \n",
    "    while True:\n",
    "        div_kern(below,above,middle)\n",
    "        comp_kern1(change_below,g_offsets,middle)\n",
    "        comp_kern2(change_above,g_offsets,middle)\n",
    "        \n",
    "        if not (gpuarray.sum(change_below+change_above).get()):\n",
    "            break\n",
    "        else:\n",
    "            below = pycuda_where(change_below, middle+1, below)\n",
    "            above = pycuda_where(change_above, middle, above)\n",
    "    \n",
    "    return middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_2 = gpuarray_search(offsets, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print\n",
    "middle_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
